{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e3c610",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Feature Engineering\n",
    "## 1.1 Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62d91816",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "#       importing packages\n",
    "########################################\n",
    "import pandas            as pd                       # data science essentials\n",
    "import matplotlib.pyplot as plt                      # data visualization\n",
    "import seaborn           as sns                      # enhanced data viz\n",
    "import numpy             as np                       # import np\n",
    "from sklearn.model_selection import train_test_split # train-test split\n",
    "from sklearn.tree import DecisionTreeRegressor       # regression trees\n",
    "from sklearn.ensemble import RandomForestRegressor   # Random Forest models\n",
    "from sklearn.ensemble import GradientBoostingRegressor # Gradient Boosting Model\n",
    "from sklearn.linear_model import LogisticRegression  # logistic regression\n",
    "import statsmodels.formula.api as smf                # logistic regression\n",
    "from sklearn.metrics import confusion_matrix         # confusion matrix\n",
    "from sklearn.metrics import roc_auc_score            # auc score\n",
    "from sklearn.neighbors import KNeighborsClassifier   # KNN for classification\n",
    "from sklearn.neighbors import KNeighborsRegressor    # KNN for regression\n",
    "from sklearn.preprocessing import StandardScaler     # standard scaler\n",
    "from sklearn.tree import DecisionTreeClassifier      # classification trees\n",
    "from sklearn.tree import plot_tree                   # tree plots\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning\n",
    "from sklearn.ensemble import GradientBoostingClassifier # gbm\n",
    "from sklearn.ensemble import AdaBoostClassifier      # Ada model\n",
    "from sklearn.ensemble import RandomForestClassifier  # Randome Forest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8346501",
   "metadata": {},
   "source": [
    "## 1.2 General Information\n",
    "### Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93d513f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5537 entries, 0 to 5536\n",
      "Data columns (total 7 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   dataset                        5537 non-null   object \n",
      " 1   wav_filename                   5537 non-null   object \n",
      " 2   start_time_s                   5537 non-null   float64\n",
      " 3   duration_s                     5537 non-null   float64\n",
      " 4   location                       5537 non-null   object \n",
      " 5   date                           5537 non-null   object \n",
      " 6   pst_or_master_tape_identifier  5537 non-null   object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 302.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "path             = \"./__datasets/\"\n",
    "training_dataset = \"train.csv\"\n",
    "\n",
    "\n",
    "# reading in the .csv file with pandas\n",
    "whale_train    = pd.read_csv(filepath_or_buffer = path + training_dataset)\n",
    "\n",
    "# checking basic info about the dataset\n",
    "whale_train.info(verbose = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1107fa7",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bf7763d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 828 entries, 0 to 827\n",
      "Data columns (total 7 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   dataset                        828 non-null    object \n",
      " 1   wav_filename                   828 non-null    object \n",
      " 2   start_time_s                   828 non-null    float64\n",
      " 3   duration_s                     828 non-null    float64\n",
      " 4   location                       828 non-null    object \n",
      " 5   date                           828 non-null    object \n",
      " 6   pst_or_master_tape_identifier  828 non-null    object \n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 45.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# importing the training dataset\n",
    "path             = \"./__datasets/\"\n",
    "training_dataset = \"test.csv\"\n",
    "\n",
    "\n",
    "# reading in the .csv file with pandas\n",
    "whale_test    = pd.read_csv(filepath_or_buffer = path + training_dataset)\n",
    "\n",
    "# checking basic info about the dataset\n",
    "whale_test.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff63257",
   "metadata": {},
   "source": [
    "## 1.3 Combine two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "558f7b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/tzb2784178b0_43z3hyw0s_00000gn/T/ipykernel_60050/807442899.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  whale_df = whale_train.append(other = whale_test)\n"
     ]
    }
   ],
   "source": [
    "whale_train['set'] = 'Training'\n",
    "whale_test ['set'] = 'Testing'\n",
    "\n",
    "# concatenating both datasets together for mv and feature engineering\n",
    "whale_df = whale_train.append(other = whale_test)\n",
    "\n",
    "# resetting index to avoid problems later in the code\n",
    "whale_df.reset_index(drop = False,\n",
    "                       inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6087f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dataset</th>\n",
       "      <th>wav_filename</th>\n",
       "      <th>start_time_s</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>location</th>\n",
       "      <th>date</th>\n",
       "      <th>pst_or_master_tape_identifier</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>34.126</td>\n",
       "      <td>2.918</td>\n",
       "      <td>Dabob Bay, Seattle, Washington</td>\n",
       "      <td>1960-10-28</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>36.816</td>\n",
       "      <td>2.588</td>\n",
       "      <td>Dabob Bay, Seattle, Washington</td>\n",
       "      <td>1960-10-28</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>42.550</td>\n",
       "      <td>2.055</td>\n",
       "      <td>Dabob Bay, Seattle, Washington</td>\n",
       "      <td>1960-10-28</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>44.606</td>\n",
       "      <td>2.410</td>\n",
       "      <td>Dabob Bay, Seattle, Washington</td>\n",
       "      <td>1960-10-28</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>46.636</td>\n",
       "      <td>3.425</td>\n",
       "      <td>Dabob Bay, Seattle, Washington</td>\n",
       "      <td>1960-10-28</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60026.wav</td>\n",
       "      <td>21.041</td>\n",
       "      <td>1.467</td>\n",
       "      <td>80 mi. south of Martha's Vineyard, Massachusetts</td>\n",
       "      <td>1960-12-21</td>\n",
       "      <td>60026</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>96</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60026.wav</td>\n",
       "      <td>25.381</td>\n",
       "      <td>2.731</td>\n",
       "      <td>80 mi. south of Martha's Vineyard, Massachusetts</td>\n",
       "      <td>1960-12-21</td>\n",
       "      <td>60026</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60026.wav</td>\n",
       "      <td>26.000</td>\n",
       "      <td>1.700</td>\n",
       "      <td>80 mi. south of Martha's Vineyard, Massachusetts</td>\n",
       "      <td>1960-12-21</td>\n",
       "      <td>60026</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>98</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60026.wav</td>\n",
       "      <td>29.000</td>\n",
       "      <td>1.700</td>\n",
       "      <td>80 mi. south of Martha's Vineyard, Massachusetts</td>\n",
       "      <td>1960-12-21</td>\n",
       "      <td>60026</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>99</td>\n",
       "      <td>podcast_round1</td>\n",
       "      <td>60026.wav</td>\n",
       "      <td>39.354</td>\n",
       "      <td>7.243</td>\n",
       "      <td>80 mi. south of Martha's Vineyard, Massachusetts</td>\n",
       "      <td>1960-12-21</td>\n",
       "      <td>60026</td>\n",
       "      <td>Training</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    index         dataset wav_filename  start_time_s  duration_s  \\\n",
       "0       0  podcast_round1    60012.wav        34.126       2.918   \n",
       "1       1  podcast_round1    60012.wav        36.816       2.588   \n",
       "2       2  podcast_round1    60012.wav        42.550       2.055   \n",
       "3       3  podcast_round1    60012.wav        44.606       2.410   \n",
       "4       4  podcast_round1    60012.wav        46.636       3.425   \n",
       "..    ...             ...          ...           ...         ...   \n",
       "95     95  podcast_round1    60026.wav        21.041       1.467   \n",
       "96     96  podcast_round1    60026.wav        25.381       2.731   \n",
       "97     97  podcast_round1    60026.wav        26.000       1.700   \n",
       "98     98  podcast_round1    60026.wav        29.000       1.700   \n",
       "99     99  podcast_round1    60026.wav        39.354       7.243   \n",
       "\n",
       "                                            location        date  \\\n",
       "0                     Dabob Bay, Seattle, Washington  1960-10-28   \n",
       "1                     Dabob Bay, Seattle, Washington  1960-10-28   \n",
       "2                     Dabob Bay, Seattle, Washington  1960-10-28   \n",
       "3                     Dabob Bay, Seattle, Washington  1960-10-28   \n",
       "4                     Dabob Bay, Seattle, Washington  1960-10-28   \n",
       "..                                               ...         ...   \n",
       "95  80 mi. south of Martha's Vineyard, Massachusetts  1960-12-21   \n",
       "96  80 mi. south of Martha's Vineyard, Massachusetts  1960-12-21   \n",
       "97  80 mi. south of Martha's Vineyard, Massachusetts  1960-12-21   \n",
       "98  80 mi. south of Martha's Vineyard, Massachusetts  1960-12-21   \n",
       "99  80 mi. south of Martha's Vineyard, Massachusetts  1960-12-21   \n",
       "\n",
       "   pst_or_master_tape_identifier       set  \n",
       "0                          60012  Training  \n",
       "1                          60012  Training  \n",
       "2                          60012  Training  \n",
       "3                          60012  Training  \n",
       "4                          60012  Training  \n",
       "..                           ...       ...  \n",
       "95                         60026  Training  \n",
       "96                         60026  Training  \n",
       "97                         60026  Training  \n",
       "98                         60026  Training  \n",
       "99                         60026  Training  \n",
       "\n",
       "[100 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whale_df.head(n=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307b45d5",
   "metadata": {},
   "source": [
    "## 1.4 Correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f739088e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "duration_s      1.00\n",
       "start_time_s    0.18\n",
       "Name: duration_s, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a correlation matrix\n",
    "whale_df_corr = whale_train.corr(method = 'pearson').round(decimals = 2)\n",
    "\n",
    "# transforming correlations to absolute values\n",
    "whale_df_corr.loc[ : , 'duration_s' ].apply(func = abs).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d968ab5f",
   "metadata": {},
   "source": [
    "## 1.5 Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4bba205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6365 entries, 0 to 6364\n",
      "Data columns (total 9 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   index                          6365 non-null   int64  \n",
      " 1   dataset                        6365 non-null   object \n",
      " 2   wav_filename                   6365 non-null   object \n",
      " 3   start_time_s                   6365 non-null   float64\n",
      " 4   duration_s                     6365 non-null   float64\n",
      " 5   location                       6365 non-null   object \n",
      " 6   date                           6365 non-null   object \n",
      " 7   pst_or_master_tape_identifier  6365 non-null   object \n",
      " 8   set                            6365 non-null   object \n",
      "dtypes: float64(2), int64(1), object(6)\n",
      "memory usage: 447.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# INFOrmation about each variable\n",
    "whale_df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e4d1a57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                            0\n",
       "dataset                          0\n",
       "wav_filename                     0\n",
       "start_time_s                     0\n",
       "duration_s                       0\n",
       "location                         0\n",
       "date                             0\n",
       "pst_or_master_tape_identifier    0\n",
       "set                              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# taking the cooking dataset\n",
    "# and then\n",
    "# transforming it into boolean based on if a value is null\n",
    "# and then\n",
    "# summing together the results per column\n",
    "whale_df.isnull().sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4e98c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making sure all missing values have been taken care of\n",
    "whale_df.isnull().any().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f867f12f",
   "metadata": {},
   "source": [
    "## 1.6 One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d1ed1f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Type of  dataset\n",
      "------\n",
      "podcast_round1         3139\n",
      "podcast_round10         623\n",
      "podcast_test_round1     523\n",
      "podcast_round3          496\n",
      "podcast_round2          446\n",
      "podcast_round9          287\n",
      "podcast_test_round3     192\n",
      "podcast_round6          176\n",
      "podcast_round7          167\n",
      "podcast_round11         118\n",
      "podcast_test_round2     113\n",
      "podcast_round12          54\n",
      "podcast_round5           31\n",
      "Name: dataset, dtype: int64\n",
      "\n",
      "\n",
      "Type of wav_filename\n",
      "-------------\n",
      "OS_7_05_2019_08_24_00_.wav                         523\n",
      "60026.wav                                          264\n",
      "64031.wav                                          178\n",
      "1562337136_0005.wav                                 38\n",
      "64030.wav                                           35\n",
      "                                                  ... \n",
      "97677001.wav                                         1\n",
      "97692001.wav                                         1\n",
      "97692002.wav                                         1\n",
      "97692003.wav                                         1\n",
      "streaming-orcasound-net_2019_11_14_13_29_00.wav      1\n",
      "Name: wav_filename, Length: 2975, dtype: int64\n",
      "\n",
      "Type of start_time_s\n",
      "-------------\n",
      "0.000000     2854\n",
      "9.800000       22\n",
      "2.450000       22\n",
      "36.750000      21\n",
      "24.500000      21\n",
      "             ... \n",
      "41.636638       1\n",
      "54.861816       1\n",
      "59.360683       1\n",
      "20.431896       1\n",
      "49.707355       1\n",
      "Name: start_time_s, Length: 3046, dtype: int64\n",
      "\n",
      "\n",
      "Type of duration_s\n",
      "-------------\n",
      "0.000000    715\n",
      "2.450000    512\n",
      "1.500000    309\n",
      "1.361111     10\n",
      "1.600356     10\n",
      "           ... \n",
      "1.994000      1\n",
      "1.732000      1\n",
      "2.364000      1\n",
      "1.605000      1\n",
      "1.566766      1\n",
      "Name: duration_s, Length: 3670, dtype: int64\n",
      "\n",
      "\n",
      "Type of location\n",
      "----------\n",
      "orcasound_lab                                                          1952\n",
      "Oregon Coast Aquarium                                                  1011\n",
      "bush_point                                                              741\n",
      "port_townsend                                                           533\n",
      "Kikvika, Norway                                                         472\n",
      "Oregon Coastal Aquarium, Newport, OR                                    437\n",
      "WHOIS                                                                   309\n",
      "Vancouver, British Columbia                                             267\n",
      "80 mi. south of Martha's Vineyard, Massachusetts                        251\n",
      "Andenes, Norway                                                         156\n",
      "Outside St. John's Hbr., Newfoundland                                    90\n",
      "Saanich Inlet, Victoria, Vancouver Island, British Columbia, Canada      54\n",
      "T3 Ice Island, Canada                                                    40\n",
      "Dabob Bay, Seattle, Washington                                           33\n",
      "80 mi. south of Martha's Vineyard, Massachusetts    X                    13\n",
      "Oregon Coastal Aquarium, Newport, OR    X                                 5\n",
      "Saanich Inlet, Victoria, VAncouver Island, British Columbia, Canada       1\n",
      "Name: location, dtype: int64 \n",
      "\n",
      "\n",
      "Type of date\n",
      "----------\n",
      "2020-09-28     561\n",
      "1562340736     523\n",
      "2017-09-27     496\n",
      "2019-07-05     446\n",
      "2020-09-29     287\n",
      "              ... \n",
      "19780-08-01      3\n",
      "1997-05-22       1\n",
      "1997-02-14       1\n",
      "1998-01-06       1\n",
      "1998-10-10       1\n",
      "Name: date, Length: 79, dtype: int64 \n",
      "\n",
      "\n",
      "Type of pst_or_master_tape_identifier\n",
      "-------------\n",
      "OS_7_05_2019_08_24_00_.wav    523\n",
      "60026                         264\n",
      "08:32:16                      222\n",
      "64031                         178\n",
      "92304                         174\n",
      "                             ... \n",
      "05:08:00                        1\n",
      "12:35:00                        1\n",
      "05:16:00                        1\n",
      "12:27:00                        1\n",
      "13:39:00                        1\n",
      "Name: pst_or_master_tape_identifier, Length: 546, dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing columns\n",
    "# checking each columns has how many category and count, for better understanding data\n",
    "print(f\"\"\"\n",
    "Type of  dataset\n",
    "------\n",
    "{whale_df['dataset'].value_counts()}\n",
    "\n",
    "\n",
    "Type of wav_filename\n",
    "-------------\n",
    "{whale_df['wav_filename'].value_counts()}\n",
    "\n",
    "Type of start_time_s\n",
    "-------------\n",
    "{whale_df['start_time_s'].value_counts()}\n",
    "\n",
    "\n",
    "Type of duration_s\n",
    "-------------\n",
    "{whale_df['duration_s'].value_counts()}\n",
    "\n",
    "\n",
    "Type of location\n",
    "----------\n",
    "{whale_df['location'].value_counts()} \n",
    "\n",
    "\n",
    "Type of date\n",
    "----------\n",
    "{whale_df['date'].value_counts()} \n",
    "\n",
    "\n",
    "Type of pst_or_master_tape_identifier\n",
    "-------------\n",
    "{whale_df['pst_or_master_tape_identifier'].value_counts()}\n",
    "\n",
    "\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac10ea4",
   "metadata": {},
   "source": [
    "### 1.6.1 Create Y-Variable\n",
    "###   ( Duration )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b476d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column for y-var\n",
    "whale_df['whale_appear'] = whale_df['duration_s'].map(lambda x: 0 if x == 0 else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23539e02",
   "metadata": {},
   "source": [
    "### 1.6.2 One-hot Encoding\n",
    "### ( Dataset & Location & Date )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab588183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding categorical variables\n",
    "one_hot_Dataset      = pd.get_dummies(whale_df['dataset'],prefix = 'dataset_')\n",
    "one_hot_Location     = pd.get_dummies(whale_df['location'],prefix = 'location_')\n",
    "one_hot_Date         = pd.get_dummies(whale_df['date'],prefix = 'date_')\n",
    "\n",
    "# dropping categorical variables after they've been encoded\n",
    "whale_df = whale_df.drop('dataset', axis = 1) # dropping even though not encoded\n",
    "whale_df = whale_df.drop('location', axis = 1)\n",
    "whale_df = whale_df.drop('date', axis = 1)\n",
    "\n",
    "# joining codings together\n",
    "whale_df = whale_df.join([one_hot_Dataset,one_hot_Location,one_hot_Date])\n",
    "\n",
    "\n",
    "# saving new columns\n",
    "new_columns= whale_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "826cf071",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>wav_filename</th>\n",
       "      <th>start_time_s</th>\n",
       "      <th>duration_s</th>\n",
       "      <th>pst_or_master_tape_identifier</th>\n",
       "      <th>set</th>\n",
       "      <th>whale_appear</th>\n",
       "      <th>dataset__podcast_round1</th>\n",
       "      <th>dataset__podcast_round10</th>\n",
       "      <th>dataset__podcast_round11</th>\n",
       "      <th>...</th>\n",
       "      <th>date__2020-09-06</th>\n",
       "      <th>date__2020-09-07</th>\n",
       "      <th>date__2020-09-08</th>\n",
       "      <th>date__2020-09-27</th>\n",
       "      <th>date__2020-09-28</th>\n",
       "      <th>date__2020-09-29</th>\n",
       "      <th>date__2020-10-08</th>\n",
       "      <th>date__2020-10-18</th>\n",
       "      <th>date__2020_09_01</th>\n",
       "      <th>date__9/27/2017</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>34.126</td>\n",
       "      <td>2.918</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>36.816</td>\n",
       "      <td>2.588</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>42.550</td>\n",
       "      <td>2.055</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>44.606</td>\n",
       "      <td>2.410</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>60012.wav</td>\n",
       "      <td>46.636</td>\n",
       "      <td>3.425</td>\n",
       "      <td>60012</td>\n",
       "      <td>Training</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 116 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index wav_filename  start_time_s  duration_s pst_or_master_tape_identifier  \\\n",
       "0      0    60012.wav        34.126       2.918                         60012   \n",
       "1      1    60012.wav        36.816       2.588                         60012   \n",
       "2      2    60012.wav        42.550       2.055                         60012   \n",
       "3      3    60012.wav        44.606       2.410                         60012   \n",
       "4      4    60012.wav        46.636       3.425                         60012   \n",
       "\n",
       "        set  whale_appear  dataset__podcast_round1  dataset__podcast_round10  \\\n",
       "0  Training             1                        1                         0   \n",
       "1  Training             1                        1                         0   \n",
       "2  Training             1                        1                         0   \n",
       "3  Training             1                        1                         0   \n",
       "4  Training             1                        1                         0   \n",
       "\n",
       "   dataset__podcast_round11  ...  date__2020-09-06  date__2020-09-07  \\\n",
       "0                         0  ...                 0                 0   \n",
       "1                         0  ...                 0                 0   \n",
       "2                         0  ...                 0                 0   \n",
       "3                         0  ...                 0                 0   \n",
       "4                         0  ...                 0                 0   \n",
       "\n",
       "   date__2020-09-08  date__2020-09-27  date__2020-09-28  date__2020-09-29  \\\n",
       "0                 0                 0                 0                 0   \n",
       "1                 0                 0                 0                 0   \n",
       "2                 0                 0                 0                 0   \n",
       "3                 0                 0                 0                 0   \n",
       "4                 0                 0                 0                 0   \n",
       "\n",
       "   date__2020-10-08  date__2020-10-18  date__2020_09_01  date__9/27/2017  \n",
       "0                 0                 0                 0                0  \n",
       "1                 0                 0                 0                0  \n",
       "2                 0                 0                 0                0  \n",
       "3                 0                 0                 0                0  \n",
       "4                 0                 0                 0                0  \n",
       "\n",
       "[5 rows x 116 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whale_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14628dfb",
   "metadata": {},
   "source": [
    "## 1.7 Range\n",
    "### Range of Duration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b3c142b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    0.42\n",
       "3    0.37\n",
       "1    0.20\n",
       "Name: Range_of_Duration, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the range of rating\n",
    "whale_df['Range_of_Duration']      = 0\n",
    "for index, value in whale_df.iterrows():\n",
    "    \n",
    "    # Conditions for ranges\n",
    "    #range3\n",
    "    if whale_df.loc[index, 'duration_s'] > 2:\n",
    "        whale_df.loc[index, 'Range_of_Duration'] = 3  \n",
    "    #range2     \n",
    "    elif whale_df.loc[index, 'duration_s'] > 1:\n",
    "        whale_df.loc[index, 'Range_of_Duration'] = 2\n",
    "     #range1     \n",
    "    elif whale_df.loc[index, 'duration_s'] >= 0:\n",
    "        whale_df.loc[index, 'Range_of_Duration'] = 1\n",
    "    # make else      \n",
    "    else:\n",
    "        whale_df.loc[index, 'Range_of_Duration'] = 'Error'\n",
    "        \n",
    "        \n",
    "# checking results\n",
    "whale_df[\"Range_of_Duration\"].value_counts(normalize = True,\n",
    "                                           sort      = True,\n",
    "                                           ascending = False).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f371f4c2",
   "metadata": {},
   "source": [
    "### Range of Start Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec079930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum start_time_s value: 1787.819\n"
     ]
    }
   ],
   "source": [
    "max_start_time = whale_df['start_time_s'].max()\n",
    "print(\"Maximum start_time_s value:\", max_start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea5d50c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.86\n",
       "3    0.07\n",
       "2    0.07\n",
       "Name: Range_of_StartTime, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# name the range of rating\n",
    "whale_df['Range_of_StartTime']      = 0\n",
    "for index, value in whale_df.iterrows():\n",
    "    \n",
    "    # Conditions for ranges\n",
    "    #range3\n",
    "    if whale_df.loc[index, 'start_time_s'] > 600:\n",
    "        whale_df.loc[index, 'Range_of_StartTime'] = 3  \n",
    "    #range2     \n",
    "    elif whale_df.loc[index, 'start_time_s'] > 180:\n",
    "        whale_df.loc[index, 'Range_of_StartTime'] = 2     \n",
    "     #range1     \n",
    "    elif whale_df.loc[index, 'start_time_s'] >= 0:\n",
    "        whale_df.loc[index, 'Range_of_StartTime'] = 1\n",
    "    # make else      \n",
    "    else:\n",
    "        whale_df.loc[index, 'Range_of_StartTime'] = 'Error'\n",
    "        \n",
    "        \n",
    "# checking results\n",
    "whale_df[\"Range_of_StartTime\"].value_counts(normalize = True,\n",
    "                                            sort      = True,\n",
    "                                            ascending = False).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a6d23c",
   "metadata": {},
   "source": [
    "## 1.7 Categorize \n",
    "### (Location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13d8a36",
   "metadata": {},
   "source": [
    "### Replacing special punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c337e811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4k/tzb2784178b0_43z3hyw0s_00000gn/T/ipykernel_60050/4025908878.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  whale_df.columns = whale_df.columns.str.replace(\"[ ,.'\\\"-/]+\", \"_\")\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the special punctuation\n",
    "whale_df.columns = whale_df.columns.str.replace(\"[ ,.'\\\"-/]+\", \"_\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660481ee",
   "metadata": {},
   "source": [
    "### U.S. Western Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcf230fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.79\n",
       "0    0.21\n",
       "Name: US_West, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"US_West\"] = whale_df.loc[ : ,  [\"location__orcasound_lab\", \"location__Oregon_Coast_Aquarium\", \"location__bush_point\", \"location__port_townsend\", \"location__Oregon_Coastal_Aquarium_Newport_OR\", \"location__Oregon_Coastal_Aquarium_Newport_OR_X\", \"location__WHOIS\", \"location__Dabob_Bay_Seattle_Washington\"]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"location__orcasound_lab\", \"location__Oregon_Coast_Aquarium\", \"location__bush_point\", \"location__port_townsend\", \"location__Oregon_Coastal_Aquarium_Newport_OR\", \"location__Oregon_Coastal_Aquarium_Newport_OR_X\", \"location__WHOIS\", \"location__Dabob_Bay_Seattle_Washington\"], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"US_West\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86a4b98",
   "metadata": {},
   "source": [
    "### U.S. Eestern Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0a8b339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.94\n",
       "1    0.06\n",
       "Name: US_Eest, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"US_Eest\"] = whale_df.loc[ : ,  [\"location__80_mi_south_of_Martha_s_Vineyard_Massachusetts\", \"location__80_mi_south_of_Martha_s_Vineyard_Massachusetts_X\", \"location__Outside_St_John_s_Hbr_Newfoundland\"]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"location__80_mi_south_of_Martha_s_Vineyard_Massachusetts\", \"location__80_mi_south_of_Martha_s_Vineyard_Massachusetts_X\", \"location__Outside_St_John_s_Hbr_Newfoundland\"], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"US_Eest\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1e237e",
   "metadata": {},
   "source": [
    "### Canada Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98699f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.94\n",
       "1    0.06\n",
       "Name: Canada, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Canada\"] = whale_df.loc[ : ,  [\"location__Vancouver_British_Columbia\", \"location__Saanich_Inlet_Victoria_VAncouver_Island_British_Columbia_Canada\", \"location__Saanich_Inlet_Victoria_Vancouver_Island_British_Columbia_Canada\", \"location__T3_Ice_Island_Canada\"]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"location__Vancouver_British_Columbia\", \"location__Saanich_Inlet_Victoria_VAncouver_Island_British_Columbia_Canada\", \"location__Saanich_Inlet_Victoria_Vancouver_Island_British_Columbia_Canada\", \"location__T3_Ice_Island_Canada\"], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Canada\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9a7ea",
   "metadata": {},
   "source": [
    "### Norway Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3f9d72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.9\n",
       "1    0.1\n",
       "Name: Norway, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Norway\"] = whale_df.loc[ : ,  [\"location__Andenes_Norway\", \"location__Kikvika_Norway\"]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"location__Andenes_Norway\", \"location__Kikvika_Norway\"], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Norway\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57098391",
   "metadata": {},
   "source": [
    "### (Podcast Rounds)\n",
    "\n",
    "### Podcast Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ae57a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_podcast_rounds  podcast_frequency\n",
      "0                        1           0.076923\n",
      "1                        1           0.076923\n",
      "2                        1           0.076923\n",
      "3                        1           0.076923\n",
      "4                        1           0.076923\n",
      "...                    ...                ...\n",
      "6360                     1           0.076923\n",
      "6361                     1           0.076923\n",
      "6362                     1           0.076923\n",
      "6363                     1           0.076923\n",
      "6364                     1           0.076923\n",
      "\n",
      "[6365 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a DataFrame named 'df' with a 'podcast' column\n",
    "# Create binary features for each podcast round\n",
    "podcast_rounds = ['dataset__podcast_round{}'.format(i) for i in [1, 2, 3, 5, 6, 7, 9, 10, 11, 12]] + ['dataset__podcast_test_round{}'.format(i) for i in range(1, 4)]\n",
    "\n",
    "# Count the total number of podcast rounds for each observation\n",
    "whale_df['total_podcast_rounds'] = whale_df[podcast_rounds].sum(axis=1)\n",
    "\n",
    "# Calculate Podcast Frequency\n",
    "whale_df['podcast_frequency'] = whale_df['total_podcast_rounds'] / len(podcast_rounds)\n",
    "\n",
    "# Display the result\n",
    "print(whale_df[['total_podcast_rounds', 'podcast_frequency']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dc8bd7",
   "metadata": {},
   "source": [
    "### (DATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166f42b",
   "metadata": {},
   "source": [
    "### 1.71 Harmonize date formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88c25f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human-readable date: 2019-07-05 15:32:16\n"
     ]
    }
   ],
   "source": [
    "# Checking the speical nub to human-readable date\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = 1562340736\n",
    "date_object = datetime.utcfromtimestamp(timestamp)\n",
    "\n",
    "print(\"Human-readable date:\", date_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "406f7d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the unix time\n",
    "whale_df.rename(columns={'date__1562340736': 'date__2019_07_05'}, inplace=True)\n",
    "\n",
    "# Chaning the spelling wrong time\n",
    "whale_df.rename(columns={'date__19610_08_01': 'date__1961_08_01'}, inplace=True)\n",
    "whale_df.rename(columns={'date__19640_08_01': 'date__1964_08_01'}, inplace=True)\n",
    "whale_df.rename(columns={'date__19650_08_01': 'date__1965_08_01'}, inplace=True)\n",
    "whale_df.rename(columns={'date__19660_08_01': 'date__1966_08_01'}, inplace=True)\n",
    "whale_df.rename(columns={'date__19780_08_01': 'date__1978_08_01'}, inplace=True)\n",
    "whale_df.rename(columns={'date__19790_08_01': 'date__1979_08_01'}, inplace=True)\n",
    "whale_df.rename(columns={'date__19670_08_01': 'date__1967_08_01'}, inplace=True)\n",
    "\n",
    "# Changing the Format reversal time\n",
    "whale_df.rename(columns={'date__9_27_2017': 'date__2017_09_27'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaef95f",
   "metadata": {},
   "source": [
    "### 1.72 Making Date to Four Seasons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd302a2",
   "metadata": {},
   "source": [
    "### Spring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fcc9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__1966_05_13\",\n",
      "\"date__1993_04_01\",\n",
      "\"date__1997_04_28\",\n",
      "\"date__1997_05_22\",\n",
      "\"date__1998_03_03\",\n",
      "\"date__1998_03_08\",\n",
      "\"date__1998_03_11\",\n",
      "\"date__1998_03_26\",\n",
      "\"date__1998_03_27\",\n",
      "\"date__1998_04_11\",\n",
      "\"date__1998_04_12\",\n",
      "\"date__1998_04_13\",\n",
      "\"date__1998_04_16\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_dates = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of March to May\n",
    "        if 3 <= date_object.month <= 5:\n",
    "            matching_dates.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_dates:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "34231d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.94\n",
       "1    0.06\n",
       "Name: Spring, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Spring\"] = whale_df.loc[ : ,  [\"date__1966_05_13\",\n",
    "\"date__1993_04_01\",\n",
    "\"date__1997_04_28\",\n",
    "\"date__1997_05_22\",\n",
    "\"date__1998_03_03\",\n",
    "\"date__1998_03_08\",\n",
    "\"date__1998_03_11\",\n",
    "\"date__1998_03_26\",\n",
    "\"date__1998_03_27\",\n",
    "\"date__1998_04_11\",\n",
    "\"date__1998_04_12\",\n",
    "\"date__1998_04_13\",\n",
    "\"date__1998_04_16\"]  ].sum(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Spring\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162aba49",
   "metadata": {},
   "source": [
    "### Summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39a100aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__2019_07_05\",\n",
      "\"date__1961_08_01\",\n",
      "\"date__1964_07_17\",\n",
      "\"date__1964_08_16\",\n",
      "\"date__1964_08_17\",\n",
      "\"date__1964_08_18\",\n",
      "\"date__1964_08_01\",\n",
      "\"date__1965_08_01\",\n",
      "\"date__1966_08_01\",\n",
      "\"date__1967_08_01\",\n",
      "\"date__1976_08_04\",\n",
      "\"date__1978_08_01\",\n",
      "\"date__1979_08_01\",\n",
      "\"date__1993_06_01\",\n",
      "\"date__1997_06_10\",\n",
      "\"date__2019_07_05\",\n",
      "\"date__2020_07_25\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_dates = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of March to May\n",
    "        if 6 <= date_object.month <= 8:\n",
    "            matching_dates.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_dates:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50a6ea7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.71\n",
       "2    0.15\n",
       "1    0.14\n",
       "Name: Summer, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Summer\"] = whale_df.loc[ : ,  [\"date__2019_07_05\",\n",
    "\"date__1961_08_01\",\n",
    "\"date__1964_07_17\",\n",
    "\"date__1964_08_16\",\n",
    "\"date__1964_08_17\",\n",
    "\"date__1964_08_18\",\n",
    "\"date__1964_08_01\",\n",
    "\"date__1965_08_01\",\n",
    "\"date__1966_08_01\",\n",
    "\"date__1967_08_01\",\n",
    "\"date__1976_08_04\",\n",
    "\"date__1978_08_01\",\n",
    "\"date__1979_08_01\",\n",
    "\"date__1993_06_01\",\n",
    "\"date__1997_06_10\",\n",
    "\"date__2019_07_05\",\n",
    "\"date__2020_07_25\"]  ].sum(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Summer\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac305635",
   "metadata": {},
   "source": [
    "### Autumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a15f5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__1960_10_28\",\n",
      "\"date__1989_09_09\",\n",
      "\"date__1997_09_09\",\n",
      "\"date__1998_10_10\",\n",
      "\"date__2017_09_27\",\n",
      "\"date__2019_11_14\",\n",
      "\"date__2020_09_05\",\n",
      "\"date__2020_09_06\",\n",
      "\"date__2020_09_07\",\n",
      "\"date__2020_09_08\",\n",
      "\"date__2020_09_27\",\n",
      "\"date__2020_09_28\",\n",
      "\"date__2020_09_29\",\n",
      "\"date__2020_10_08\",\n",
      "\"date__2020_10_18\",\n",
      "\"date__2020_09_01\",\n",
      "\"date__2017_09_27\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_dates = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of March to May\n",
    "        if 9 <= date_object.month <= 11:\n",
    "            matching_dates.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_dates:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af990752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.62\n",
       "1    0.29\n",
       "2    0.10\n",
       "Name: Autumn, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Autumn\"] = whale_df.loc[ : ,  [\"date__1960_10_28\",\n",
    "\"date__1989_09_09\",\n",
    "\"date__1997_09_09\",\n",
    "\"date__1998_10_10\",\n",
    "\"date__2017_09_27\",\n",
    "\"date__2019_11_14\",\n",
    "\"date__2020_09_05\",\n",
    "\"date__2020_09_06\",\n",
    "\"date__2020_09_07\",\n",
    "\"date__2020_09_08\",\n",
    "\"date__2020_09_27\",\n",
    "\"date__2020_09_28\",\n",
    "\"date__2020_09_29\",\n",
    "\"date__2020_10_08\",\n",
    "\"date__2020_10_18\",\n",
    "\"date__2020_09_01\",\n",
    "\"date__2017_09_27\"]  ].sum(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Autumn\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83999b1e",
   "metadata": {},
   "source": [
    "### Winter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d233fbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__1958_02_19\",\n",
      "\"date__1960_12_21\",\n",
      "\"date__1992_12_29\",\n",
      "\"date__1992_12_30\",\n",
      "\"date__1997_01_01\",\n",
      "\"date__1997_02_12\",\n",
      "\"date__1997_02_13\",\n",
      "\"date__1997_02_14\",\n",
      "\"date__1997_02_16\",\n",
      "\"date__1997_02_18\",\n",
      "\"date__1997_02_21\",\n",
      "\"date__1998_01_06\",\n",
      "\"date__1998_01_08\",\n",
      "\"date__1998_01_09\",\n",
      "\"date__1998_01_12\",\n",
      "\"date__1998_01_13\",\n",
      "\"date__1998_01_14\",\n",
      "\"date__1998_01_15\",\n",
      "\"date__1998_01_16\",\n",
      "\"date__1998_01_17\",\n",
      "\"date__1998_01_18\",\n",
      "\"date__1998_01_19\",\n",
      "\"date__1998_01_20\",\n",
      "\"date__1998_01_29\",\n",
      "\"date__1998_01_30\",\n",
      "\"date__1998_01_31\",\n",
      "\"date__1998_02_17\",\n",
      "\"date__1998_02_19\",\n",
      "\"date__1998_02_20\",\n",
      "\"date__1998_02_21\",\n",
      "\"date__1998_02_22\",\n",
      "\"date__1998_02_28\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_dates = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of March to May\n",
    "        if date_object.month in [12, 1, 2]:\n",
    "            matching_dates.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_dates:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf0703c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.73\n",
       "1    0.27\n",
       "Name: Winter, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Winter\"] = whale_df.loc[ : ,  [\"date__1958_02_19\",\n",
    "\"date__1960_12_21\",\n",
    "\"date__1992_12_29\",\n",
    "\"date__1992_12_30\",\n",
    "\"date__1997_01_01\",\n",
    "\"date__1997_02_12\",\n",
    "\"date__1997_02_13\",\n",
    "\"date__1997_02_14\",\n",
    "\"date__1997_02_16\",\n",
    "\"date__1997_02_18\",\n",
    "\"date__1997_02_21\",\n",
    "\"date__1998_01_06\",\n",
    "\"date__1998_01_08\",\n",
    "\"date__1998_01_09\",\n",
    "\"date__1998_01_12\",\n",
    "\"date__1998_01_13\",\n",
    "\"date__1998_01_14\",\n",
    "\"date__1998_01_15\",\n",
    "\"date__1998_01_16\",\n",
    "\"date__1998_01_17\",\n",
    "\"date__1998_01_18\",\n",
    "\"date__1998_01_19\",\n",
    "\"date__1998_01_20\",\n",
    "\"date__1998_01_29\",\n",
    "\"date__1998_01_30\",\n",
    "\"date__1998_01_31\",\n",
    "\"date__1998_02_17\",\n",
    "\"date__1998_02_19\",\n",
    "\"date__1998_02_20\",\n",
    "\"date__1998_02_21\",\n",
    "\"date__1998_02_22\",\n",
    "\"date__1998_02_28\"]  ].sum(axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Winter\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cbbad8",
   "metadata": {},
   "source": [
    "### 1.73 Making Date to Year Range\n",
    "Short description:\n",
    "\n",
    "'Early_Studies': (1958, 1970),  # Early years of killer whale research\n",
    "  \n",
    "  'Captivity_Boom': (1971, 1990),  # Rise of killer whale captivity\n",
    "  \n",
    "  'Conservation_Era': (1991, 2005),  # Increased focus on killer whale conservation\n",
    " \n",
    " 'Current_Era': (2006, 2020),  # Recent years"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c8dd82",
   "metadata": {},
   "source": [
    "### Early Studies (1958 - 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "942db4c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__1958_02_19\",\n",
      "\"date__1960_10_28\",\n",
      "\"date__1960_12_21\",\n",
      "\"date__1961_08_01\",\n",
      "\"date__1964_07_17\",\n",
      "\"date__1964_08_16\",\n",
      "\"date__1964_08_17\",\n",
      "\"date__1964_08_18\",\n",
      "\"date__1964_08_01\",\n",
      "\"date__1965_08_01\",\n",
      "\"date__1966_05_13\",\n",
      "\"date__1966_08_01\",\n",
      "\"date__1967_08_01\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_years = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of 1958 to 1970\n",
    "        if 1958 <= date_object.year <= 1970:\n",
    "            matching_years.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_years:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b376fb4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.85\n",
       "1    0.15\n",
       "Name: Early_Studies, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Early_Studies\"] = whale_df.loc[ : ,  [\"date__1958_02_19\",\n",
    "\"date__1960_10_28\",\n",
    "\"date__1960_12_21\",\n",
    "\"date__1961_08_01\",\n",
    "\"date__1964_07_17\",\n",
    "\"date__1964_08_16\",\n",
    "\"date__1964_08_17\",\n",
    "\"date__1964_08_18\",\n",
    "\"date__1964_08_01\",\n",
    "\"date__1965_08_01\",\n",
    "\"date__1966_05_13\",\n",
    "\"date__1966_08_01\",\n",
    "\"date__1967_08_01\"]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"date__1958_02_19\",\n",
    "\"date__1960_10_28\",\n",
    "\"date__1960_12_21\",\n",
    "\"date__1961_08_01\",\n",
    "\"date__1964_07_17\",\n",
    "\"date__1964_08_16\",\n",
    "\"date__1964_08_17\",\n",
    "\"date__1964_08_18\",\n",
    "\"date__1964_08_01\",\n",
    "\"date__1965_08_01\",\n",
    "\"date__1966_05_13\",\n",
    "\"date__1966_08_01\",\n",
    "\"date__1967_08_01\"], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Early_Studies\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e1a70f",
   "metadata": {},
   "source": [
    "### Captivity_Boom (1971 - 1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39208e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__1976_08_04\",\n",
      "\"date__1978_08_01\",\n",
      "\"date__1979_08_01\",\n",
      "\"date__1989_09_09\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_years = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of 1958 to 1970\n",
    "        if 1971 <= date_object.year <= 1990:\n",
    "            matching_years.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_years:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a4b018ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.96\n",
       "1    0.04\n",
       "Name: Captivity_Boom, dtype: float64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Captivity_Boom\"] = whale_df.loc[ : ,  [\"date__1976_08_04\",\n",
    "\"date__1978_08_01\",\n",
    "\"date__1979_08_01\",\n",
    "\"date__1989_09_09\"]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"date__1976_08_04\",\n",
    "\"date__1978_08_01\",\n",
    "\"date__1979_08_01\",\n",
    "\"date__1989_09_09\"], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Captivity_Boom\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dfdcb0",
   "metadata": {},
   "source": [
    "### Conservation Era (1991- 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae521600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__1992_12_29\",\n",
      "\"date__1992_12_30\",\n",
      "\"date__1993_04_01\",\n",
      "\"date__1993_06_01\",\n",
      "\"date__1997_01_01\",\n",
      "\"date__1997_02_12\",\n",
      "\"date__1997_02_13\",\n",
      "\"date__1997_02_14\",\n",
      "\"date__1997_02_16\",\n",
      "\"date__1997_02_18\",\n",
      "\"date__1997_02_21\",\n",
      "\"date__1997_04_28\",\n",
      "\"date__1997_05_22\",\n",
      "\"date__1997_06_10\",\n",
      "\"date__1997_09_09\",\n",
      "\"date__1998_01_06\",\n",
      "\"date__1998_01_08\",\n",
      "\"date__1998_01_09\",\n",
      "\"date__1998_01_12\",\n",
      "\"date__1998_01_13\",\n",
      "\"date__1998_01_14\",\n",
      "\"date__1998_01_15\",\n",
      "\"date__1998_01_16\",\n",
      "\"date__1998_01_17\",\n",
      "\"date__1998_01_18\",\n",
      "\"date__1998_01_19\",\n",
      "\"date__1998_01_20\",\n",
      "\"date__1998_01_29\",\n",
      "\"date__1998_01_30\",\n",
      "\"date__1998_01_31\",\n",
      "\"date__1998_02_17\",\n",
      "\"date__1998_02_19\",\n",
      "\"date__1998_02_20\",\n",
      "\"date__1998_02_21\",\n",
      "\"date__1998_02_22\",\n",
      "\"date__1998_02_28\",\n",
      "\"date__1998_03_03\",\n",
      "\"date__1998_03_08\",\n",
      "\"date__1998_03_11\",\n",
      "\"date__1998_03_26\",\n",
      "\"date__1998_03_27\",\n",
      "\"date__1998_04_11\",\n",
      "\"date__1998_04_12\",\n",
      "\"date__1998_04_13\",\n",
      "\"date__1998_04_16\",\n",
      "\"date__1998_10_10\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_years = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of 1958 to 1970\n",
    "        if 1991 <= date_object.year <= 2005:\n",
    "            matching_years.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_years:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f994d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7\n",
       "1    0.3\n",
       "Name: Conservation_Era, dtype: float64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Conservation_Era\"] = whale_df.loc[ : ,  [\"date__1992_12_29\",\n",
    "\"date__1992_12_30\",\n",
    "\"date__1993_04_01\",\n",
    "\"date__1993_06_01\",\n",
    "\"date__1997_01_01\",\n",
    "\"date__1997_02_12\",\n",
    "\"date__1997_02_13\",\n",
    "\"date__1997_02_14\",\n",
    "\"date__1997_02_16\",\n",
    "\"date__1997_02_18\",\n",
    "\"date__1997_02_21\",\n",
    "\"date__1997_04_28\",\n",
    "\"date__1997_05_22\",\n",
    "\"date__1997_06_10\",\n",
    "\"date__1997_09_09\",\n",
    "\"date__1998_01_06\",\n",
    "\"date__1998_01_08\",\n",
    "\"date__1998_01_09\",\n",
    "\"date__1998_01_12\",\n",
    "\"date__1998_01_13\",\n",
    "\"date__1998_01_14\",\n",
    "\"date__1998_01_15\",\n",
    "\"date__1998_01_16\",\n",
    "\"date__1998_01_17\",\n",
    "\"date__1998_01_18\",\n",
    "\"date__1998_01_19\",\n",
    "\"date__1998_01_20\",\n",
    "\"date__1998_01_29\",\n",
    "\"date__1998_01_30\",\n",
    "\"date__1998_01_31\",\n",
    "\"date__1998_02_17\",\n",
    "\"date__1998_02_19\",\n",
    "\"date__1998_02_20\",\n",
    "\"date__1998_02_21\",\n",
    "\"date__1998_02_22\",\n",
    "\"date__1998_02_28\",\n",
    "\"date__1998_03_03\",\n",
    "\"date__1998_03_08\",\n",
    "\"date__1998_03_11\",\n",
    "\"date__1998_03_26\",\n",
    "\"date__1998_03_27\",\n",
    "\"date__1998_04_11\",\n",
    "\"date__1998_04_12\",\n",
    "\"date__1998_04_13\",\n",
    "\"date__1998_04_16\",\n",
    "\"date__1998_10_10\" ]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"date__1992_12_29\",\n",
    "\"date__1992_12_30\",\n",
    "\"date__1993_04_01\",\n",
    "\"date__1993_06_01\",\n",
    "\"date__1997_01_01\",\n",
    "\"date__1997_02_12\",\n",
    "\"date__1997_02_13\",\n",
    "\"date__1997_02_14\",\n",
    "\"date__1997_02_16\",\n",
    "\"date__1997_02_18\",\n",
    "\"date__1997_02_21\",\n",
    "\"date__1997_04_28\",\n",
    "\"date__1997_05_22\",\n",
    "\"date__1997_06_10\",\n",
    "\"date__1997_09_09\",\n",
    "\"date__1998_01_06\",\n",
    "\"date__1998_01_08\",\n",
    "\"date__1998_01_09\",\n",
    "\"date__1998_01_12\",\n",
    "\"date__1998_01_13\",\n",
    "\"date__1998_01_14\",\n",
    "\"date__1998_01_15\",\n",
    "\"date__1998_01_16\",\n",
    "\"date__1998_01_17\",\n",
    "\"date__1998_01_18\",\n",
    "\"date__1998_01_19\",\n",
    "\"date__1998_01_20\",\n",
    "\"date__1998_01_29\",\n",
    "\"date__1998_01_30\",\n",
    "\"date__1998_01_31\",\n",
    "\"date__1998_02_17\",\n",
    "\"date__1998_02_19\",\n",
    "\"date__1998_02_20\",\n",
    "\"date__1998_02_21\",\n",
    "\"date__1998_02_22\",\n",
    "\"date__1998_02_28\",\n",
    "\"date__1998_03_03\",\n",
    "\"date__1998_03_08\",\n",
    "\"date__1998_03_11\",\n",
    "\"date__1998_03_26\",\n",
    "\"date__1998_03_27\",\n",
    "\"date__1998_04_11\",\n",
    "\"date__1998_04_12\",\n",
    "\"date__1998_04_13\",\n",
    "\"date__1998_04_16\",\n",
    "\"date__1998_10_10\" ] , axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Conservation_Era\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae03604",
   "metadata": {},
   "source": [
    "### Current Era (2006 - 2020) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8a62b6f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"date__2019_07_05\",\n",
      "\"date__2017_09_27\",\n",
      "\"date__2019_07_05\",\n",
      "\"date__2019_11_14\",\n",
      "\"date__2020_07_25\",\n",
      "\"date__2020_09_05\",\n",
      "\"date__2020_09_06\",\n",
      "\"date__2020_09_07\",\n",
      "\"date__2020_09_08\",\n",
      "\"date__2020_09_27\",\n",
      "\"date__2020_09_28\",\n",
      "\"date__2020_09_29\",\n",
      "\"date__2020_10_08\",\n",
      "\"date__2020_10_18\",\n",
      "\"date__2020_09_01\",\n",
      "\"date__2017_09_27\",\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming whale_df is your DataFrame\n",
    "# Create a list to store the dates in the desired format\n",
    "matching_years = []\n",
    "\n",
    "# Iterate over the columns of the DataFrame\n",
    "for column in whale_df.columns:\n",
    "    # Check if the column name starts with 'date__'\n",
    "    if column.startswith('date__'):\n",
    "        # Extract the date part from the column name\n",
    "        date_str = column.replace('date__', '')\n",
    "        \n",
    "        # Convert the date string to a datetime object\n",
    "        date_object = datetime.strptime(date_str, '%Y_%m_%d')\n",
    "        \n",
    "        # Check if the date is in the range of 1958 to 1970\n",
    "        if 2006 <= date_object.year <= 2020:\n",
    "            matching_years.append(date_object)\n",
    "\n",
    "# Print the matching dates\n",
    "for date_object in matching_years:\n",
    "    print(date_object.strftime('\"date__%Y_%m_%d\",'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26537696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.49\n",
       "1    0.26\n",
       "2    0.25\n",
       "Name: Current_Era, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combing all us. west cities\n",
    "whale_df[\"Current_Era\"] = whale_df.loc[ : ,  [\"date__2019_07_05\",\n",
    "\"date__2017_09_27\",\n",
    "\"date__2019_07_05\",\n",
    "\"date__2019_11_14\",\n",
    "\"date__2020_07_25\",\n",
    "\"date__2020_09_05\",\n",
    "\"date__2020_09_06\",\n",
    "\"date__2020_09_07\",\n",
    "\"date__2020_09_08\",\n",
    "\"date__2020_09_27\",\n",
    "\"date__2020_09_28\",\n",
    "\"date__2020_09_29\",\n",
    "\"date__2020_10_08\",\n",
    "\"date__2020_10_18\",\n",
    "\"date__2020_09_01\",\n",
    "\"date__2017_09_27\" ]  ].sum(axis = 1)\n",
    "\n",
    "whale_df = whale_df.drop([\"date__2019_07_05\",\n",
    "\"date__2017_09_27\",\n",
    "\"date__2019_07_05\",\n",
    "\"date__2019_11_14\",\n",
    "\"date__2020_07_25\",\n",
    "\"date__2020_09_05\",\n",
    "\"date__2020_09_06\",\n",
    "\"date__2020_09_07\",\n",
    "\"date__2020_09_08\",\n",
    "\"date__2020_09_27\",\n",
    "\"date__2020_09_28\",\n",
    "\"date__2020_09_29\",\n",
    "\"date__2020_10_08\",\n",
    "\"date__2020_10_18\",\n",
    "\"date__2020_09_01\",\n",
    "\"date__2017_09_27\" ], axis = 1)\n",
    "\n",
    "# checking results\n",
    "whale_df[\"Current_Era\"].value_counts(normalize = True).round(decimals = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8dd3b",
   "metadata": {},
   "source": [
    "# 2. Model preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02892b79",
   "metadata": {},
   "source": [
    "## 2.1 Checking each column type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3493c2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6365 entries, 0 to 6364\n",
      "Data columns (total 36 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   index                          6365 non-null   int64  \n",
      " 1   wav_filename                   6365 non-null   object \n",
      " 2   start_time_s                   6365 non-null   float64\n",
      " 3   duration_s                     6365 non-null   float64\n",
      " 4   pst_or_master_tape_identifier  6365 non-null   object \n",
      " 5   set                            6365 non-null   object \n",
      " 6   whale_appear                   6365 non-null   int64  \n",
      " 7   dataset__podcast_round1        6365 non-null   uint8  \n",
      " 8   dataset__podcast_round10       6365 non-null   uint8  \n",
      " 9   dataset__podcast_round11       6365 non-null   uint8  \n",
      " 10  dataset__podcast_round12       6365 non-null   uint8  \n",
      " 11  dataset__podcast_round2        6365 non-null   uint8  \n",
      " 12  dataset__podcast_round3        6365 non-null   uint8  \n",
      " 13  dataset__podcast_round5        6365 non-null   uint8  \n",
      " 14  dataset__podcast_round6        6365 non-null   uint8  \n",
      " 15  dataset__podcast_round7        6365 non-null   uint8  \n",
      " 16  dataset__podcast_round9        6365 non-null   uint8  \n",
      " 17  dataset__podcast_test_round1   6365 non-null   uint8  \n",
      " 18  dataset__podcast_test_round2   6365 non-null   uint8  \n",
      " 19  dataset__podcast_test_round3   6365 non-null   uint8  \n",
      " 20  Range_of_Duration              6365 non-null   int64  \n",
      " 21  Range_of_StartTime             6365 non-null   int64  \n",
      " 22  US_West                        6365 non-null   int64  \n",
      " 23  US_Eest                        6365 non-null   int64  \n",
      " 24  Canada                         6365 non-null   int64  \n",
      " 25  Norway                         6365 non-null   int64  \n",
      " 26  total_podcast_rounds           6365 non-null   int64  \n",
      " 27  podcast_frequency              6365 non-null   float64\n",
      " 28  Spring                         6365 non-null   int64  \n",
      " 29  Summer                         6365 non-null   int64  \n",
      " 30  Autumn                         6365 non-null   int64  \n",
      " 31  Winter                         6365 non-null   int64  \n",
      " 32  Early_Studies                  6365 non-null   int64  \n",
      " 33  Captivity_Boom                 6365 non-null   int64  \n",
      " 34  Conservation_Era               6365 non-null   int64  \n",
      " 35  Current_Era                    6365 non-null   int64  \n",
      "dtypes: float64(3), int64(17), object(3), uint8(13)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# INFOrmation about each variable\n",
    "whale_df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980fae9",
   "metadata": {},
   "source": [
    "## 2.2 Cleaning sepcial punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0356e2f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " index + \n",
      " wav_filename + \n",
      " start_time_s + \n",
      " duration_s + \n",
      " pst_or_master_tape_identifier + \n",
      " set + \n",
      " whale_appear + \n",
      " dataset__podcast_round1 + \n",
      " dataset__podcast_round10 + \n",
      " dataset__podcast_round11 + \n",
      " dataset__podcast_round12 + \n",
      " dataset__podcast_round2 + \n",
      " dataset__podcast_round3 + \n",
      " dataset__podcast_round5 + \n",
      " dataset__podcast_round6 + \n",
      " dataset__podcast_round7 + \n",
      " dataset__podcast_round9 + \n",
      " dataset__podcast_test_round1 + \n",
      " dataset__podcast_test_round2 + \n",
      " dataset__podcast_test_round3 + \n",
      " Range_of_Duration + \n",
      " Range_of_StartTime + \n",
      " US_West + \n",
      " US_Eest + \n",
      " Canada + \n",
      " Norway + \n",
      " total_podcast_rounds + \n",
      " podcast_frequency + \n",
      " Spring + \n",
      " Summer + \n",
      " Autumn + \n",
      " Winter + \n",
      " Early_Studies + \n",
      " Captivity_Boom + \n",
      " Conservation_Era + \n",
      " Current_Era + \n"
     ]
    }
   ],
   "source": [
    "for val in whale_df:\n",
    "    print(f\" {val} + \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "caabd02a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.206679\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zxx/opt/anaconda3/lib/python3.9/site-packages/statsmodels/base/model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>   <td>0.412</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>   <td>whale_appear</td>         <td>AIC:</td>        <td>2639.0294</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2023-11-16 18:11</td>       <td>BIC:</td>        <td>2666.0637</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>        <td>6365</td>        <td>Log-Likelihood:</td>   <td>-1315.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>3</td>            <td>LL-Null:</td>       <td>-2236.4</td> \n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>          <td>6361</td>         <td>LLR p-value:</td>     <td>0.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>0.0000</td>           <td>Scale:</td>        <td>1.0000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>35.0000</td>             <td></td>              <td></td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>               <th>Coef.</th>   <th>Std.Err.</th>       <th>z</th>     <th>P>|z|</th>    <th>[0.025</th>      <th>0.975]</th>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>1.4634</td>    <td>0.0499</td>     <td>29.3182</td> <td>0.0000</td>    <td>1.3655</td>      <td>1.5612</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>start_time_s</th>              <td>3.6484</td>    <td>0.7962</td>     <td>4.5822</td>  <td>0.0000</td>    <td>2.0878</td>      <td>5.2089</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dataset__podcast_round10</th>  <td>-3.9105</td>   <td>0.3330</td>    <td>-11.7432</td> <td>0.0000</td>    <td>-4.5632</td>     <td>-3.2579</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>dataset__podcast_round11</th> <td>-29.9679</td> <td>142477.6828</td>  <td>-0.0002</td> <td>0.9998</td> <td>-279281.0949</td> <td>279221.1590</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                                    Results: Logit\n",
       "======================================================================================\n",
       "Model:                      Logit                   Pseudo R-squared:        0.412    \n",
       "Dependent Variable:         whale_appear            AIC:                     2639.0294\n",
       "Date:                       2023-11-16 18:11        BIC:                     2666.0637\n",
       "No. Observations:           6365                    Log-Likelihood:          -1315.5  \n",
       "Df Model:                   3                       LL-Null:                 -2236.4  \n",
       "Df Residuals:               6361                    LLR p-value:             0.0000   \n",
       "Converged:                  0.0000                  Scale:                   1.0000   \n",
       "No. Iterations:             35.0000                                                   \n",
       "--------------------------------------------------------------------------------------\n",
       "                          Coef.     Std.Err.     z     P>|z|     [0.025       0.975]  \n",
       "--------------------------------------------------------------------------------------\n",
       "Intercept                  1.4634      0.0499  29.3182 0.0000       1.3655      1.5612\n",
       "start_time_s               3.6484      0.7962   4.5822 0.0000       2.0878      5.2089\n",
       "dataset__podcast_round10  -3.9105      0.3330 -11.7432 0.0000      -4.5632     -3.2579\n",
       "dataset__podcast_round11 -29.9679 142477.6828  -0.0002 0.9998 -279281.0949 279221.1590\n",
       "======================================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiating a logistic regression model object\n",
    "logistic_full = smf.logit(formula = \"\"\"   whale_appear ~\n",
    " start_time_s +\n",
    " dataset__podcast_round10 +\n",
    " dataset__podcast_round11 \n",
    "\n",
    " \"\"\",\n",
    "                                        data    = whale_df)\n",
    "\n",
    "\n",
    "# fitting the model object\n",
    "results_full = logistic_full.fit()\n",
    "\n",
    "\n",
    "# checking the results SUMMARY\n",
    "results_full.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcd4b0e",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7a653",
   "metadata": {},
   "source": [
    "### Setting Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b919cf8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 'index',  \n",
      " 'wav_filename',  \n",
      " 'start_time_s',  \n",
      " 'duration_s',  \n",
      " 'pst_or_master_tape_identifier',  \n",
      " 'set',  \n",
      " 'whale_appear',  \n",
      " 'dataset__podcast_round1',  \n",
      " 'dataset__podcast_round10',  \n",
      " 'dataset__podcast_round11',  \n",
      " 'dataset__podcast_round12',  \n",
      " 'dataset__podcast_round2',  \n",
      " 'dataset__podcast_round3',  \n",
      " 'dataset__podcast_round5',  \n",
      " 'dataset__podcast_round6',  \n",
      " 'dataset__podcast_round7',  \n",
      " 'dataset__podcast_round9',  \n",
      " 'dataset__podcast_test_round1',  \n",
      " 'dataset__podcast_test_round2',  \n",
      " 'dataset__podcast_test_round3',  \n",
      " 'Range_of_Duration',  \n",
      " 'Range_of_StartTime',  \n",
      " 'US_West',  \n",
      " 'US_Eest',  \n",
      " 'Canada',  \n",
      " 'Norway',  \n",
      " 'total_podcast_rounds',  \n",
      " 'podcast_frequency',  \n",
      " 'Spring',  \n",
      " 'Summer',  \n",
      " 'Autumn',  \n",
      " 'Winter',  \n",
      " 'Early_Studies',  \n",
      " 'Captivity_Boom',  \n",
      " 'Conservation_Era',  \n",
      " 'Current_Era',  \n"
     ]
    }
   ],
   "source": [
    "for val in whale_df:\n",
    "    print(f\" '{val}',  \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7a4de41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explanatory sets from last session\n",
    "\n",
    "# creating a dictionary to store candidate models\n",
    "\n",
    "candidate_dict = {\n",
    "\n",
    " # the first x variables set for orignal coloums only\n",
    " 'x_var'   : [  \n",
    " 'start_time_s', \n",
    " 'dataset__podcast_round1',     \n",
    " 'dataset__podcast_round12',  \n",
    " 'dataset__podcast_round2',  \n",
    " 'dataset__podcast_round3',   \n",
    " 'dataset__podcast_round6',  \n",
    " 'dataset__podcast_round7',  \n",
    " 'dataset__podcast_round9'\n",
    "],\n",
    "\n",
    "    \n",
    " # set 2 x-variables (Testing Accuracy:99.13%, AUC: 98.69%, Gap:0.22%)\n",
    " 'x_var1'    : [\n",
    " 'start_time_s',\n",
    " 'dataset__podcast_round1',  \n",
    " 'dataset__podcast_round10',  \n",
    " 'dataset__podcast_round11',  \n",
    " \n",
    " 'dataset__podcast_round2',  \n",
    " 'dataset__podcast_round3',  \n",
    " \n",
    " 'dataset__podcast_round6',  \n",
    " 'dataset__podcast_round7',  \n",
    " 'dataset__podcast_round9',    \n",
    " 'US_West',  \n",
    " 'US_Eest',  \n",
    " 'Canada',  \n",
    " 'Norway',\n",
    " 'Spring',  \n",
    " 'Summer',  \n",
    " 'Autumn',  \n",
    " 'Winter',\n",
    " 'Early_Studies',  \n",
    " 'Captivity_Boom',  \n",
    " 'Conservation_Era',  \n",
    " 'Current_Era'],\n",
    "    \n",
    " # set 3 x-variables (Testing Accuracy:95.38%, AUC: 79.87%, Gap:0.25%)\n",
    " 'x_var2'    : [\n",
    " 'Range_of_StartTime',\n",
    " 'dataset__podcast_round1',  \n",
    " 'dataset__podcast_round10',  \n",
    " 'dataset__podcast_round11',  \n",
    " \n",
    " 'dataset__podcast_round2',  \n",
    " 'dataset__podcast_round3',  \n",
    " \n",
    " 'dataset__podcast_round6',  \n",
    " 'dataset__podcast_round7',  \n",
    " 'dataset__podcast_round9',    \n",
    " 'US_West',  \n",
    " 'US_Eest',  \n",
    " 'Canada',  \n",
    " 'Norway',  \n",
    " 'Spring',  \n",
    " 'Summer',  \n",
    " 'Autumn',  \n",
    " 'Winter']\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf18201a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring explanatory variables\n",
    "whale_df_data   = whale_df.drop(['whale_appear'], axis=1)\n",
    "whale_df_data   =  whale_df.loc[ : , candidate_dict['x_var2']]\n",
    "\n",
    "# setting explanatory variable(s) with most correlated x-variable\n",
    "x_train = whale_df[candidate_dict['x_var2']] [whale_df['set'] == 'Training' ]\n",
    "\n",
    "# setting response variable\n",
    "\n",
    "y_train = whale_df[ 'whale_appear' ][ whale_df['set']   == 'Training' ]\n",
    "\n",
    "# developing training and validation sets\n",
    "x_train_1, x_train_2, y_train_1, y_train_2 = train_test_split(\n",
    "            x_train,\n",
    "            y_train.astype(dtype = 'int'),\n",
    "            random_state = 123,\n",
    "            test_size    = 0.25,\n",
    "            stratify     = y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a44f8d8",
   "metadata": {},
   "source": [
    "## 4.1 Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bde4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:   0.9513\n",
      "Testing Accuracy:    0.9538\n",
      "Training AUC:        0.7883\n",
      "Testing AUC:         0.7987\n",
      "Gap   :              0.0025\n"
     ]
    }
   ],
   "source": [
    "# INSTANTIATING a classification tree object\n",
    "tree_model = DecisionTreeClassifier()\n",
    "\n",
    "\n",
    "# FITTING to the training data\n",
    "tree_model_fit = tree_model.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "tree_model_train_pred = tree_model_fit.predict(x_train_1)\n",
    "tree_model_valid_pred = tree_model_fit.predict(x_train_2)\n",
    "\n",
    "\n",
    "# SCORING the results (accuracy)\n",
    "tree_model_train_score = tree_model.score(x_train_1, y_train_1).round(4) # training accuracy\n",
    "tree_model_valid_score = tree_model.score(x_train_2, y_train_2).round(4) # validation accuracy\n",
    "\n",
    "# SCORING the results (auc)\n",
    "tree_model_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = tree_model_train_pred).round(decimals = 4)\n",
    "\n",
    "tree_model_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = tree_model_valid_pred).round(decimals = 4)\n",
    "\n",
    "# displaying results\n",
    "print('Training Accuracy:  ', tree_model_train_score)\n",
    "print('Testing Accuracy:   ', tree_model_valid_score)\n",
    "print('Training AUC:       ', tree_model_train_auc)\n",
    "print('Testing AUC:        ', tree_model_valid_auc)\n",
    "\n",
    "# print the gap\n",
    "tree_model_gap = abs(tree_model_train_score - tree_model_valid_score).round(4)\n",
    "print('Gap   :             ', tree_model_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82adb79b",
   "metadata": {},
   "source": [
    "## 4.2 Gradient Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "188f2ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:   0.9424\n",
      "Validation Accuracy: 0.9415\n",
      "Training AUC:        0.7495\n",
      "Validation AUC:      0.7453\n",
      "Gap :                0.0009\n"
     ]
    }
   ],
   "source": [
    "#INSTANTIATING Tuned Model\n",
    "gbc_model = GradientBoostingClassifier()\n",
    "\n",
    "# FITTING to the training data\n",
    "gbc_model_fit = gbc_model.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "gbc_model_train_pred = gbc_model_fit.predict(x_train_1)\n",
    "gbc_model_valid_pred = gbc_model_fit.predict(x_train_2)\n",
    "\n",
    "\n",
    "# SCORING the results (accuracy)\n",
    "gbc_model_train_score = gbc_model.score(x_train_1, y_train_1).round(4) # training accuracy\n",
    "gbc_model_valid_score = gbc_model.score(x_train_2, y_train_2).round(4) # validation accuracy\n",
    "\n",
    "# SCORING the results (auc)\n",
    "gbc_model_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = gbc_model_train_pred).round(decimals = 4)\n",
    "\n",
    "gbc_model_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = gbc_model_valid_pred).round(decimals = 4)\n",
    "\n",
    "# displaying results\n",
    "print('Training Accuracy:  ', gbc_model_train_score)\n",
    "print('Validation Accuracy:', gbc_model_valid_score)\n",
    "print('Training AUC:       ', gbc_model_train_auc)\n",
    "print('Validation AUC:     ', gbc_model_valid_auc)\n",
    "\n",
    "# print the gap\n",
    "gbc_model_gap = abs(gbc_model_train_score - gbc_model_valid_score).round(4)\n",
    "print('Gap :               ', gbc_model_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4660d",
   "metadata": {},
   "source": [
    "## 4.3. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42dbbe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:   0.6131\n",
      "Validation Accuracy: 0.6234\n",
      "Training AUC:        0.9538\n",
      "Validation AUC:      0.9551\n",
      "Gap :                0.0103\n"
     ]
    }
   ],
   "source": [
    "#INSTANTIATING Tuned Model\n",
    "model = RandomForestRegressor(n_estimators     = 200,\n",
    "                              criterion        = 'squared_error',\n",
    "                              max_depth        = 30,\n",
    "                              min_samples_leaf = 10,\n",
    "                              bootstrap        = True,\n",
    "                              warm_start       = False,\n",
    "                              random_state     = 219)\n",
    "\n",
    "# FITTING to the training data\n",
    "model_fit = model.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "model_train_pred = model_fit.predict(x_train_1)\n",
    "model_valid_pred = model_fit.predict(x_train_2)\n",
    "\n",
    "\n",
    "# SCORING the results (accuracy)\n",
    "model_train_score = model.score(x_train_1, y_train_1).round(4) # training accuracy\n",
    "model_valid_score = model.score(x_train_2, y_train_2).round(4) # validation accuracy\n",
    "\n",
    "# SCORING the results (auc)\n",
    "model_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = model_train_pred).round(decimals = 4)\n",
    "\n",
    "model_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = model_valid_pred).round(decimals = 4)\n",
    "\n",
    "# displaying results\n",
    "print('Training Accuracy:  ', model_train_score)\n",
    "print('Validation Accuracy:', model_valid_score)\n",
    "print('Training AUC:       ', model_train_auc)\n",
    "print('Validation AUC:     ', model_valid_auc)\n",
    "\n",
    "# print the gap\n",
    "model_gap = abs(model_train_score - model_valid_score).round(4)\n",
    "print('Gap :               ', model_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d4c85",
   "metadata": {},
   "source": [
    "## 4.4 Unpruned GBR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "175783a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:   0.5216\n",
      "Validation Accuracy: 0.5453\n",
      "Training AUC:        0.9528\n",
      "Validation AUC:      0.9527\n",
      "Gap :                0.0237\n"
     ]
    }
   ],
   "source": [
    "#INSTANTIATING Tuned Model\n",
    "rf_model = GradientBoostingRegressor(loss          = 'huber',\n",
    "                                  learning_rate = 0.1,\n",
    "                                  n_estimators  = 150,\n",
    "                                  criterion     = 'friedman_mse',\n",
    "                                  max_depth     = 8,\n",
    "                                  warm_start    = False,\n",
    "                                  random_state  = 219)\n",
    "\n",
    "# FITTING to the training data\n",
    "rf_model_fit = rf_model.fit(x_train_1, y_train_1)\n",
    "\n",
    "\n",
    "# PREDICTING on the response variable\n",
    "rf_model_train_pred = rf_model_fit.predict(x_train_1)\n",
    "rf_model_valid_pred = rf_model_fit.predict(x_train_2)\n",
    "\n",
    "\n",
    "# SCORING the results (accuracy)\n",
    "rf_model_train_score = rf_model.score(x_train_1, y_train_1).round(4) # training accuracy\n",
    "rf_model_valid_score = rf_model.score(x_train_2, y_train_2).round(4) # validation accuracy\n",
    "\n",
    "# SCORING the results (auc)\n",
    "rf_model_train_auc = roc_auc_score(y_true  = y_train_1,\n",
    "                                y_score = rf_model_train_pred).round(decimals = 4)\n",
    "\n",
    "rf_model_valid_auc = roc_auc_score(y_true  = y_train_2,\n",
    "                                y_score = rf_model_valid_pred).round(decimals = 4)\n",
    "\n",
    "# displaying results\n",
    "print('Training Accuracy:  ', rf_model_train_score)\n",
    "print('Validation Accuracy:', rf_model_valid_score)\n",
    "print('Training AUC:       ', rf_model_train_auc)\n",
    "print('Validation AUC:     ', rf_model_valid_auc)\n",
    "\n",
    "# print the gap\n",
    "rf_model_gap = abs(rf_model_train_score - rf_model_valid_score).round(4)\n",
    "print('Gap :               ', rf_model_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732cf26f",
   "metadata": {},
   "source": [
    "## Short Sum: \n",
    "\n",
    "### Predicting Killer Whale Appearances\n",
    "\n",
    "This model aims to predict the appearances of killer whales based on a comprehensive set of factors. To enhance the model's predictive power, new features have been engineered. \n",
    "\n",
    "Key features include the range of start times, podcast rounds, geographical locations (US West, US East, Canada, Norway), seasonal information (Spring, Summer, Autumn, Winter), historical periods (Early Studies, Captivity Boom, Conservation Era, Current Era), and podcast frequency.\n",
    "\n",
    "The ultimate goal is to create a robust predictive tool that contributes to our understanding and forecasting of killer whale appearances, incorporating insights from various dimensions including temporal, geographical, and historical contexts.\n",
    "\n",
    "---\n",
    "The Best Model is __Decision Tree Model.__\n",
    "\n",
    "About the X-var decision:\n",
    "\n",
    "1. If the goal is to achieve the highest overall accuracy, Set 2 might be preferred.\n",
    "\n",
    "2. If the goal is for better interpretability or avoiding overfitting, and if the validation metrics are acceptable, Set 3 could be a reasonable choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddf7074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
